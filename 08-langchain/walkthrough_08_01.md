* Open the terminal panel (*Ctrl-\`*) and install the `langchain` and `langchain-groq` packages:
    ```bash
    pip install langchain langchain-groq
    ```
* In `08-langchain` create a new file called `simple.py`
* Import the `os` model to retrieve the API key
    ```python
    import os
    ```
* You'll create a prompt using a template.  Import the `PromptTemplate` type from the `langchain.prompts` module
    ```python
    from langchain.prompts import PromptTemplate
    ```
* You'll use the **Llama 3.3 70B** from Groq to generate the response.  Import the `ChatGroq` type from the `langchain_groq` module
    ```python
    from langchain_groq import ChatGroq
    ```
* And you'll need the `StrOutputParser` from the `langchain_core.output_parsers` module
    ```python
    from langchain_core.output_parsers import StrOutputParser
    ```
* Use the `from_template` function to generate a prompt from a template.  Notice that the template has a variable for a topic.  It also specifies the format of the response.
    ```python
    prompt = PromptTemplate.from_template(
        """
        List three pros and cons of the following topic:

        Topic: {topic}

        Format as:
        Pros:
        - ...
        Cons:
        - ...

        Restrict the length of each pro and con to 20 words or less.
        """
    )
    ```
* The `ChatGroq` class will accept the name of the Groq model to use to generate the response and the `GROQ_API_KEY` secret.  Also, set the temperature to 0.  This is a good practice for structured outputs.
    ```python
    llm = ChatGroq(
        model="llama-3.3-70b-versatile",
        api_key=os.getenv("GROQ_API_KEY"),
        temperature=0
    )
    ```
* The `StrOutputParser` will display the content from the first message generated by the `llm`.
    ```python
    parser = StrOutputParser()
    ```
* Now form a `chain` using the pipe operator to direct the output of the `template`, `llm` and `parser`.
    ```python
    chain = prompt | llm | parser
    ```
* To generate the response, `invoke` the chain.  Pass the `invoke` method a dictionary with a key/value pair for each variable in the `template`.
    ```python
    response = chain.invoke({"topic": "artificial intelligence"})
    ```
* And now you can `print` the `response`
    ```python
    print(response)
    ```
* Install the `langchain-openai` package
    ```bash
    pip install langchain-openai
    ```
* Import the `ChatOpenAI` type from the `langchain_openai` module
    ```python
    from langchain_openai import ChatOpenAI
    ```
* Replace the `ChatGroq` `llm` with a `ChatOpenAI` instance using the base URL, API key, and model from `03-github-models`
    ```python
    llm = ChatOpenAI(
        openai_api_key=os.getenv("GH_MODELS_TOKEN"),
        openai_api_base="https://models.github.ai/inference",
        model_name="openai/gpt-4.1-mini"
    )
    ```
